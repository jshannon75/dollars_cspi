---
title: "R Notebook"
output: html_notebook
editor_options: 
  chunk_output_type: console
---

Check botscore here: https://antcpt.com/score_detector/

```{r setup}
library(RSelenium)
library(purrr)
library(tidyverse)
library(rvest)
library(XML)
library(httr)
library(reticulate)

#Open browser for scraping
#binman::list_versions("chromedriver")
# rD <- rsDriver(browser = "chrome",chromever = NULL,port=9515L)
# rD <- rsDriver(browser = "firefox",chromever = NULL)
# remDr <- rD$client
  # remDr$close()
  # rD$server$stop()

```

Get a list of the state store pages

```{r}

#Get xpaths for state pages in the store directory
xpath_list<-data.frame(part1="/html/body/div[4]/div/div[2]/div/div[2]/div[2]/div[",
                       part2=1:50,part3="]/p/a") %>%
  unite(col="xpath",c(part1,part2,part3),sep="")

state_url_list<-function(xpath_sel){
  state_page<-remDr$findElement(using="xpath",
                                value=xpath_sel)

  data.frame(state_url=state_page$getElementAttribute("href")[[1]])
}

state_urls<-map_df(xpath_list$xpath,state_url_list)



```

Get a list of cities with Dollar Generals in each state. This function pulls all the URLs from each state page, filtering for just those pointing to a specific town.

```{r}
ga_urls<-state_urls %>% filter(str_detect(state_url,"ga"))

#url_sel<-as.character(ga_urls)

store_link_scrape<-function(url_sel){
  remDr$deleteAllCookies()
  remDr$navigate(url_sel)
  
  page<-remDr$getPageSource()[[1]] %>%
  read_html() 
  
  doc <- htmlTreeParse(page, useInternal=T)
  links_state<-data.frame(urls=xpathSApply(doc, "//a[@href]", xmlGetAttr, "href"))#%>%
    #filter(str_detect(urls,"https") & str_detect(urls,"locations"))
}


city_pages<-map_df(ga_urls$state_url,store_link_scrape) 

city_pages_url<-city_pages %>%
  separate(urls,into=c("a","pageend"),sep="en/") %>%
  filter(str_detect(pageend,"directory/")) %>%
  mutate(city_url=paste("https://www.dollargeneral.com/",pageend,sep=""))


###NOW SAVED TO THIS POINT
write_csv(city_pages_url,"data/dg_citypages_url_ga.csv")

```

Get store links for just Georgia (in this case)--done in Python

```{python}
from selenium import webdriver
import pandas as pd
from lxml import html
import csv
import undetected_chromedriver as uc
import time

# Assuming you have already loaded city_pages_url using pandas read_csv
city_pages_url = pd.read_csv("data/dg_citypages_url_ga.csv")


def store_link_scrape(url_sel):
    # Initialize the WebDriver (make sure you have the appropriate webdriver installed)
    options = uc.ChromeOptions() 
    options.add_argument("--headless")
    options.add_argument("--allow-insecure-localhost")
    options.add_argument("--window-size=1400x1400")
    options.add_argument("--disable-gpu")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument ("--profiler-timing=0") # whether chrome will contain timing information
    options.add_argument ("--disable-infobars") # prevent infobars from appearing
    options.add_argument ("--headless") # headless on chrome
    options.add_argument ("--blink-settings=imagesEnabled=false") # disable images
    options.add_argument ("--no-referrers") # don't send HTTP-Referer headers
    options.add_argument ("--disable-breakpad") # disables the crash reporting
    options.add_argument ("--disable-demo-model") # disables the chrome OS demo
    options.add_argument ("--disable-translate") # disable google translate
    options.add_argument ("--dns-prefetch-disable") # disable DNS prefetching
    
    driver = uc.Chrome(options=options)  # You can replac
    
    # Navigate to the URL
    driver.get(url_sel)
    
    # Get the page source
    page_source = driver.page_source
    
    # Parse the HTML page
    doc = html.fromstring(page_source)
    
    # Extract the links
    links_state = pd.DataFrame({'urls': doc.xpath("//a[@href]/@href")})
    
    # Close the WebDriver
    driver.quit()
    
    return links_state

# Apply the store_link_scrape function to each URL in city_pages_url
store_pages_ga = pd.concat(map(store_link_scrape, city_pages_url['city_url']))

store_pages_ga.to_csv("data/storepages_ga.csv",index=False)

```

(OLD CODE IN R)
```{r}
library(RSelenium)
city_pages_url<-read_csv("data/dg_citypages_url_ga.csv")

store_link_scrape1<-function(url_sel){
  #remDr$deleteAllCookies()
  remDr$navigate(url_sel)
  sleep_period=runif(1,min=2,max=5) #Vary the wait times to avoid bot detection
  
  Sys.sleep(sleep_period)
  
  page<-remDr$getPageSource()[[1]] %>%
  read_html() 
  Sys.sleep(sleep_period)
  
  doc <- htmlTreeParse(page, useInternal=T)
  links_state<-data.frame(urls=xpathSApply(doc, "//a[@href]", xmlGetAttr, "href")) #%>%
    #filter(str_detect(urls,"store-directory/")) #%>%
    #filter(urls!="/content/dollargeneral/en/store-directory/ga")
  links_state
}

# city_pages_url_10<-city_pages_url %>%
#   filter(row_number(.)<10)
# 
# testurl<-as.character(city_pages_url_10[1,]$city_url)
# test<-store_link_scrape1(testurl) 


store_pages_ga<-map_df(city_pages_url[1:3,]$city_url,store_link_scrape1)

store_pages_ga1<-store_pages_ga %>%
  filter(urls!="/content/dollargeneral/en/store-directory/ga")

write_csv(store_pages_ga1,"data/storepages_ga1.csv")

store_pages_ga1<-store_pages_ga %>%
  filter(urls!="/content/dollargeneral/en/store-directory/ga") %>%
  separate(urls,into=c("a","pageend"),sep="/en/",remove=FALSE) %>%
  filter(str_detect(pageend,"directory/")) %>%
  mutate(store_url=paste("https://www.dollargeneral.com/",pageend,sep="")) %>%
  separate(store_url,into=c("city_url","store"),sep="/[0-9]",remove=FALSE) %>%
  select(-store)

city_pages_url_redo<-city_pages_url %>%
  anti_join(store_pages_ga1,by="city_url")

stores_pages_ga_redo<-map_df(city_pages_url_redo$city_url,store_link_scrape1)


```

Finalize city pages in R:
```{r}
store_pages_ga1 <-read_csv("data/storepages_ga.csv")%>%
  filter(urls!="/content/dollargeneral/en/store-directory/ga") %>%
  separate(urls,into=c("a","pageend"),sep="/en/",remove=FALSE) %>%
  filter(str_detect(pageend,"directory/")) %>%
  mutate(store_url=paste("https://www.dollargeneral.com/",pageend,sep="")) %>%
  separate(store_url,into=c("city_url","store"),sep="/[0-9]",remove=FALSE) %>%
  select(-store)

write_csv(store_pages_ga1 %>% select(store_url,city_url),
          "data/dg_storepages_python_2024_02_24.csv")

city_url<-read_csv("data/dg_citypages_url_ga.csv") 

city_match<-city_url %>%
  anti_join(store_pages_ga1 %>% select(city_url))

```

Load store pages
```{python}
import pandas as pd
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.firefox.options import Options
from bs4 import BeautifulSoup
import time
import random

# Read CSV file
store_pages = pd.read_csv("data/dg_storepages_python_2024_02_24.csv")[1:5]

    options = uc.ChromeOptions() 
    options.add_argument("--allow-insecure-localhost")
    options.add_argument("--window-size=1400x1400")
    #options.add_argument("--disable-gpu")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--deny-permission-prompts")
    options.add_argument ("--profiler-timing=0") # whether chrome will contain timing information
    #options.add_argument ("--disable-infobars") # prevent infobars from appearing
    #options.add_argument ("--headless") # headless on chrome
    options.add_argument ("--blink-settings=imagesEnabled=false") # disable images
    options.add_argument ("--no-referrers") # don't send HTTP-Referer headers
    options.add_argument ("--disable-breakpad") # disables the crash reporting
    options.add_argument ("--disable-demo-model") # disables the chrome OS demo
    options.add_argument ("--disable-translate") # disable google translate
    options.add_argument ("--dns-prefetch-disable") # disable DNS prefetching
    
    driver = uc.Chrome(options=options)  
    driver = uc.Chrome()  
    #driver = webdriver.Firefox(options=options)
    #driver = webdriver.Firefox()

    driver.execute_cdp_cmd(
      "Browser.grantPermissions",
      {
          "origin": "https://www.openstreetmap.org/",
          "permissions": ["geolocation"],
      },
    )

# Select a random URL
url_sel = random.choice(store_pages['store_url'])

driver.get('https://www.dollargeneral.com/store-directory/mn/minneapolis/22315')

    # Store info
st_address = driver.find_element(By.XPATH, '//*[@id="main-content-id"]/div/div/div[1]/div[1]/h1').text

def url_extract(url_sel):
    driver.get(url_sel)

    # Store info
    st_address = driver.find_element(By.XPATH, '//*[@id="main-content-id"]/div/div/div[1]/div[1]/h1').text
    city = driver.find_element(By.XPATH, '//*[@id="main-content-id"]/div/div/div[1]/div[1]/div').text
    lat = driver.find_element(By.XPATH, '//*[@id="main-content-id"]/div/div/div[1]/div[2]').get_attribute('data-latitude')
    long = driver.find_element(By.XPATH, '//*[@id="main-content-id"]/div/div/div[1]/div[2]').get_attribute('data-longitude')

    # Store services
    #services = driver.find_element(By.XPATH, '/html/body/div[4]/div/div[2]/div/div/div[5]/div[2]/ul')
    #service_items = services.find_elements(By.XPATH, 'li')

    #def list_extract(item_sel):
    #    value_xsel = f'/html/body/div[4]/div/div[2]/div/div/div[5]/div[2]/ul/li[{item_sel}]/span'
    #    value_item = driver.find_element(By.XPATH, value_xsel)
    #    return value_item.text

    services_data = pd.DataFrame({
        #'store_service': [list_extract(item_sel) for item_sel in range(1, len(service_items) + 1)],
        'address': [st_address],
        'city': [city],
        # 'lat': [lat],
        # 'long': [long]
    })

    return services_data

# Apply the function to each URL and concatenate the results
ga_services = pd.concat([url_extract(url) for url in store_pages['store_url']])

# Close the browser
driver.quit()

```



Extract store pages:


```{r}
# store_pages<-read_csv("data/dg_store_links.csv") %>%
#   mutate(store_url2=str_replace(store_url,
#                     "https://www.dollargeneral.com/content/dollargeneral/en/store-directory/","")) %>%
#   separate(store_url2,sep="/",into=c("state","city","store_id"),
#            remove=FALSE)

#Open browser for scrpaing
store_pages<-read_csv("data/dg_storepages_python_2024_02_24.csv")

exCap <- list("moz:firefoxOptions" = list(
      "prefs" = list(
        "geo.enabled" = FALSE #Allow for location tracking in the browser before running.
      ),
      args = list('--headless')))

rD <- rsDriver(browser = "firefox",
               chromever = NULL)#,
               #extraCapabilities=exCap)
remDr <- rD$client
remDr$close()
rD$server$stop()

url_extract<-function(url_sel){
  url_sep<-separate(data.frame(url_temp=
                                 str_replace(url_sel,
                                  "https://www.dollargeneral.com/content/dollargeneral/en/store-directory/","")),
                    col=url_temp,sep="/",into=c("a","b","c","store_id"))

  remDr$navigate(url_sel)
  Sys.sleep(runif(1,min=6.2,max=9.5))
  
  #Store info
  st_address_x<-remDr$findElement(using="xpath",
                                value="/html/body/div[4]/div/div[2]/div/div/div[1]/div[1]/h1")
  st_address<-st_address_x$getElementText()[[1]]
  
  city_x<-remDr$findElement(using="xpath",
                            value="/html/body/div[4]/div/div[2]/div/div/div[1]/div[1]/div")
  city<-city_x$getElementText()[[1]]
  
  latlong_x<-remDr$findElement(using="xpath",
                             value="/html/body/div[4]/div/div[2]/div/div/div[1]/div[2]")
  lat<-latlong_x$getElementAttribute(attrName = "data-latitude")[[1]]
  long<-latlong_x$getElementAttribute(attrName = "data-longitude")[[1]]
  
  #Store services
  services <- remDr$findElement(using = "xpath",
                                 value = '/html/body/div[4]/div/div[2]/div/div/div[5]/div[2]/ul')
  list_count<-length(services$findChildElements(using="xpath",value="li"))

  list_extract<-function(item_sel){
    value_xsel<-paste("/html/body/div[4]/div/div[2]/div/div/div[5]/div[2]/ul/li[",item_sel,"]/span",sep="")
    value_item<-remDr$findElement(using = "xpath",
                                  value = value_xsel)
    value_item$getElementText()[[1]]
  }
  
  data.frame(store_service=t(data.frame(map(1:list_count,list_extract))),
                     row.names=NULL) %>%
    mutate(store_url=url_sel,
           address=st_address,
           city=city,
           lat=lat,
           long=long)
}

ga_services<-map_df(store_pages$store_url,possibly(url_extract))

check<-anti_join(ga,ga_services)

ga_services2<-map_df(check$store_url,possibly(url_extract))

check2<-anti_join(check,ga_services2)

ga_services3<-map_df(check2$store_url,possibly(url_extract))

check3<-anti_join(check2,ga_services3)

ga_services_all<-ga_services %>%
  bind_rows(ga_services2) %>%
  bind_rows(ga_services3) %>%
  mutate(dummy=1) %>%
  pivot_wider(names_from=store_service,values_from=dummy,values_fill=0) %>%
  right_join(ga) %>%
  select(store_url,state,address,city,city_url,store_id,lat,long,everything()) %>%
  select(-store_url2)

write_csv(ga_services_all,"data/georgia_sample_2023_12_02.csv")

```

Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Ctrl+Alt+I*.

When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Ctrl+Shift+K* to preview the HTML file).

The preview shows you a rendered HTML copy of the contents of the editor. Consequently, unlike *Knit*, *Preview* does not run any R code chunks. Instead, the output of the chunk when it was last run in the editor is displayed.
